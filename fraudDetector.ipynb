{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['months_as_customer', 'age', 'policy_number', 'policy_bind_date',\n",
       "       'policy_state', 'policy_csl', 'policy_deductable',\n",
       "       'policy_annual_premium', 'umbrella_limit', 'insured_zip', 'insured_sex',\n",
       "       'insured_education_level', 'insured_occupation', 'insured_hobbies',\n",
       "       'insured_relationship', 'capital-gains', 'capital-loss',\n",
       "       'incident_date', 'incident_type', 'collision_type', 'incident_severity',\n",
       "       'authorities_contacted', 'incident_state', 'incident_city',\n",
       "       'incident_location', 'incident_hour_of_the_day',\n",
       "       'number_of_vehicles_involved', 'property_damage', 'bodily_injuries',\n",
       "       'witnesses', 'police_report_available', 'total_claim_amount',\n",
       "       'injury_claim', 'property_claim', 'vehicle_claim', 'auto_make',\n",
       "       'auto_model', 'auto_year', 'fraud_reported'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame = pd.read_excel(\"dataset/insurance_claims_data.xlsx\")\n",
    "\n",
    "data_frame.head()\n",
    "data_frame.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>months_as_customer</th>\n",
       "      <th>age</th>\n",
       "      <th>policy_number</th>\n",
       "      <th>policy_deductable</th>\n",
       "      <th>policy_annual_premium</th>\n",
       "      <th>umbrella_limit</th>\n",
       "      <th>insured_zip</th>\n",
       "      <th>capital-gains</th>\n",
       "      <th>capital-loss</th>\n",
       "      <th>incident_hour_of_the_day</th>\n",
       "      <th>number_of_vehicles_involved</th>\n",
       "      <th>bodily_injuries</th>\n",
       "      <th>witnesses</th>\n",
       "      <th>total_claim_amount</th>\n",
       "      <th>injury_claim</th>\n",
       "      <th>property_claim</th>\n",
       "      <th>vehicle_claim</th>\n",
       "      <th>auto_year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1.000000e+03</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.00000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>203.954000</td>\n",
       "      <td>38.948000</td>\n",
       "      <td>546238.648000</td>\n",
       "      <td>1136.000000</td>\n",
       "      <td>1256.406150</td>\n",
       "      <td>1.101000e+06</td>\n",
       "      <td>501214.488000</td>\n",
       "      <td>25126.100000</td>\n",
       "      <td>-26793.700000</td>\n",
       "      <td>11.644000</td>\n",
       "      <td>1.83900</td>\n",
       "      <td>0.992000</td>\n",
       "      <td>1.487000</td>\n",
       "      <td>52761.94000</td>\n",
       "      <td>7433.420000</td>\n",
       "      <td>7399.570000</td>\n",
       "      <td>37928.950000</td>\n",
       "      <td>2005.103000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>115.113174</td>\n",
       "      <td>9.140287</td>\n",
       "      <td>257063.005276</td>\n",
       "      <td>611.864673</td>\n",
       "      <td>244.167395</td>\n",
       "      <td>2.297407e+06</td>\n",
       "      <td>71701.610941</td>\n",
       "      <td>27872.187708</td>\n",
       "      <td>28104.096686</td>\n",
       "      <td>6.951373</td>\n",
       "      <td>1.01888</td>\n",
       "      <td>0.820127</td>\n",
       "      <td>1.111335</td>\n",
       "      <td>26401.53319</td>\n",
       "      <td>4880.951853</td>\n",
       "      <td>4824.726179</td>\n",
       "      <td>18886.252893</td>\n",
       "      <td>6.015861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>100804.000000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>433.330000</td>\n",
       "      <td>-1.000000e+06</td>\n",
       "      <td>430104.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-111100.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>100.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>1995.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>115.750000</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>335980.250000</td>\n",
       "      <td>500.000000</td>\n",
       "      <td>1089.607500</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>448404.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-51500.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>41812.50000</td>\n",
       "      <td>4295.000000</td>\n",
       "      <td>4445.000000</td>\n",
       "      <td>30292.500000</td>\n",
       "      <td>2000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>199.500000</td>\n",
       "      <td>38.000000</td>\n",
       "      <td>533135.000000</td>\n",
       "      <td>1000.000000</td>\n",
       "      <td>1257.200000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>466445.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-23250.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>58055.00000</td>\n",
       "      <td>6775.000000</td>\n",
       "      <td>6750.000000</td>\n",
       "      <td>42100.000000</td>\n",
       "      <td>2005.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>276.250000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>759099.750000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>1415.695000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>603251.000000</td>\n",
       "      <td>51025.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>3.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>70592.50000</td>\n",
       "      <td>11305.000000</td>\n",
       "      <td>10885.000000</td>\n",
       "      <td>50822.500000</td>\n",
       "      <td>2010.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>479.000000</td>\n",
       "      <td>64.000000</td>\n",
       "      <td>999435.000000</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>2047.590000</td>\n",
       "      <td>1.000000e+07</td>\n",
       "      <td>620962.000000</td>\n",
       "      <td>100500.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>4.00000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>114920.00000</td>\n",
       "      <td>21450.000000</td>\n",
       "      <td>23670.000000</td>\n",
       "      <td>79560.000000</td>\n",
       "      <td>2015.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       months_as_customer          age  policy_number  policy_deductable  \\\n",
       "count         1000.000000  1000.000000    1000.000000        1000.000000   \n",
       "mean           203.954000    38.948000  546238.648000        1136.000000   \n",
       "std            115.113174     9.140287  257063.005276         611.864673   \n",
       "min              0.000000    19.000000  100804.000000         500.000000   \n",
       "25%            115.750000    32.000000  335980.250000         500.000000   \n",
       "50%            199.500000    38.000000  533135.000000        1000.000000   \n",
       "75%            276.250000    44.000000  759099.750000        2000.000000   \n",
       "max            479.000000    64.000000  999435.000000        2000.000000   \n",
       "\n",
       "       policy_annual_premium  umbrella_limit    insured_zip  capital-gains  \\\n",
       "count            1000.000000    1.000000e+03    1000.000000    1000.000000   \n",
       "mean             1256.406150    1.101000e+06  501214.488000   25126.100000   \n",
       "std               244.167395    2.297407e+06   71701.610941   27872.187708   \n",
       "min               433.330000   -1.000000e+06  430104.000000       0.000000   \n",
       "25%              1089.607500    0.000000e+00  448404.500000       0.000000   \n",
       "50%              1257.200000    0.000000e+00  466445.500000       0.000000   \n",
       "75%              1415.695000    0.000000e+00  603251.000000   51025.000000   \n",
       "max              2047.590000    1.000000e+07  620962.000000  100500.000000   \n",
       "\n",
       "        capital-loss  incident_hour_of_the_day  number_of_vehicles_involved  \\\n",
       "count    1000.000000               1000.000000                   1000.00000   \n",
       "mean   -26793.700000                 11.644000                      1.83900   \n",
       "std     28104.096686                  6.951373                      1.01888   \n",
       "min   -111100.000000                  0.000000                      1.00000   \n",
       "25%    -51500.000000                  6.000000                      1.00000   \n",
       "50%    -23250.000000                 12.000000                      1.00000   \n",
       "75%         0.000000                 17.000000                      3.00000   \n",
       "max         0.000000                 23.000000                      4.00000   \n",
       "\n",
       "       bodily_injuries    witnesses  total_claim_amount  injury_claim  \\\n",
       "count      1000.000000  1000.000000          1000.00000   1000.000000   \n",
       "mean          0.992000     1.487000         52761.94000   7433.420000   \n",
       "std           0.820127     1.111335         26401.53319   4880.951853   \n",
       "min           0.000000     0.000000           100.00000      0.000000   \n",
       "25%           0.000000     1.000000         41812.50000   4295.000000   \n",
       "50%           1.000000     1.000000         58055.00000   6775.000000   \n",
       "75%           2.000000     2.000000         70592.50000  11305.000000   \n",
       "max           2.000000     3.000000        114920.00000  21450.000000   \n",
       "\n",
       "       property_claim  vehicle_claim    auto_year  \n",
       "count     1000.000000    1000.000000  1000.000000  \n",
       "mean      7399.570000   37928.950000  2005.103000  \n",
       "std       4824.726179   18886.252893     6.015861  \n",
       "min          0.000000      70.000000  1995.000000  \n",
       "25%       4445.000000   30292.500000  2000.000000  \n",
       "50%       6750.000000   42100.000000  2005.000000  \n",
       "75%      10885.000000   50822.500000  2010.000000  \n",
       "max      23670.000000   79560.000000  2015.000000  "
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have looked at the meta data. Let's clean this data and encode it to make it suitable for training a DecisionTreeClassifier.\n",
    "My approach to normalize this data is:\n",
    "- Drop all null valued data points.\n",
    "- Convert features with type of timestamp into a meaning full categorical feature.\n",
    "- Encode every categorical feature to make it Training ready.\n",
    "- Reduce the variance for features to the maximum extend possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's begin with removing all the rows that have null/no values in them\n",
    "data_frame = data_frame.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       months_as_customer          age  policy_number  policy_deductable  \\\n",
      "count         1000.000000  1000.000000    1000.000000        1000.000000   \n",
      "mean           203.954000    38.948000  546238.648000        1136.000000   \n",
      "std            115.113174     9.140287  257063.005276         611.864673   \n",
      "min              0.000000    19.000000  100804.000000         500.000000   \n",
      "25%            115.750000    32.000000  335980.250000         500.000000   \n",
      "50%            199.500000    38.000000  533135.000000        1000.000000   \n",
      "75%            276.250000    44.000000  759099.750000        2000.000000   \n",
      "max            479.000000    64.000000  999435.000000        2000.000000   \n",
      "\n",
      "       policy_annual_premium  umbrella_limit    insured_zip  capital-gains  \\\n",
      "count            1000.000000    1.000000e+03    1000.000000    1000.000000   \n",
      "mean             1256.406150    1.101000e+06  501214.488000   25126.100000   \n",
      "std               244.167395    2.297407e+06   71701.610941   27872.187708   \n",
      "min               433.330000   -1.000000e+06  430104.000000       0.000000   \n",
      "25%              1089.607500    0.000000e+00  448404.500000       0.000000   \n",
      "50%              1257.200000    0.000000e+00  466445.500000       0.000000   \n",
      "75%              1415.695000    0.000000e+00  603251.000000   51025.000000   \n",
      "max              2047.590000    1.000000e+07  620962.000000  100500.000000   \n",
      "\n",
      "        capital-loss  incident_hour_of_the_day  number_of_vehicles_involved  \\\n",
      "count    1000.000000               1000.000000                   1000.00000   \n",
      "mean   -26793.700000                 11.644000                      1.83900   \n",
      "std     28104.096686                  6.951373                      1.01888   \n",
      "min   -111100.000000                  0.000000                      1.00000   \n",
      "25%    -51500.000000                  6.000000                      1.00000   \n",
      "50%    -23250.000000                 12.000000                      1.00000   \n",
      "75%         0.000000                 17.000000                      3.00000   \n",
      "max         0.000000                 23.000000                      4.00000   \n",
      "\n",
      "       bodily_injuries    witnesses  total_claim_amount  injury_claim  \\\n",
      "count      1000.000000  1000.000000          1000.00000   1000.000000   \n",
      "mean          0.992000     1.487000         52761.94000   7433.420000   \n",
      "std           0.820127     1.111335         26401.53319   4880.951853   \n",
      "min           0.000000     0.000000           100.00000      0.000000   \n",
      "25%           0.000000     1.000000         41812.50000   4295.000000   \n",
      "50%           1.000000     1.000000         58055.00000   6775.000000   \n",
      "75%           2.000000     2.000000         70592.50000  11305.000000   \n",
      "max           2.000000     3.000000        114920.00000  21450.000000   \n",
      "\n",
      "       property_claim  vehicle_claim    auto_year  \n",
      "count     1000.000000    1000.000000  1000.000000  \n",
      "mean      7399.570000   37928.950000  2005.103000  \n",
      "std       4824.726179   18886.252893     6.015861  \n",
      "min          0.000000      70.000000  1995.000000  \n",
      "25%       4445.000000   30292.500000  2000.000000  \n",
      "50%       6750.000000   42100.000000  2005.000000  \n",
      "75%      10885.000000   50822.500000  2010.000000  \n",
      "max      23670.000000   79560.000000  2015.000000  \n"
     ]
    }
   ],
   "source": [
    "print(data_frame.describe())\n",
    "#this would give us all continous features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['policy_state', 'policy_csl', 'insured_sex', 'insured_education_level',\n",
       "       'insured_occupation', 'insured_hobbies', 'insured_relationship',\n",
       "       'incident_type', 'collision_type', 'incident_severity',\n",
       "       'authorities_contacted', 'incident_state', 'incident_city',\n",
       "       'incident_location', 'property_damage', 'police_report_available',\n",
       "       'auto_make', 'auto_model', 'fraud_reported'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets see which features are categorical:\n",
    "\n",
    "categorical_features = data_frame.select_dtypes(include=['object', 'category'])\n",
    "categorical_features.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the columns that were returned within the variable categorical_features, are to be encoded."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets perfom some encoding for better fitting."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are only three states, let encode this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   months_as_customer  age  policy_number policy_bind_date  policy_deductable  \\\n",
      "0                 328   48         521585       2014-10-17               1000   \n",
      "1                 228   42         342868       2006-06-27               2000   \n",
      "2                 134   29         687698       2000-09-06               2000   \n",
      "3                 256   41         227811       1990-05-25               2000   \n",
      "4                 228   44         367455       2014-06-06               1000   \n",
      "\n",
      "   policy_annual_premium  umbrella_limit  insured_zip  capital-gains  \\\n",
      "0                1406.91               0       466132          53300   \n",
      "1                1197.22         5000000       468176              0   \n",
      "2                1413.14         5000000       430632          35100   \n",
      "3                1415.74         6000000       608117          48900   \n",
      "4                1583.91         6000000       610706          66000   \n",
      "\n",
      "   capital-loss  ... 1137  1138  1139  1140  1141  1142  1143  1144  1145  \\\n",
      "0             0  ...  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "1             0  ...  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   \n",
      "2             0  ...  0.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   \n",
      "3        -62400  ...  0.0   0.0   0.0   1.0   0.0   0.0   0.0   0.0   0.0   \n",
      "4        -46000  ...  1.0   0.0   0.0   0.0   0.0   0.0   0.0   0.0   1.0   \n",
      "\n",
      "   1146  \n",
      "0   1.0  \n",
      "1   1.0  \n",
      "2   0.0  \n",
      "3   1.0  \n",
      "4   0.0  \n",
      "\n",
      "[5 rows x 1167 columns]\n",
      "Index(['months_as_customer', 'age', 'policy_number', 'policy_bind_date',\n",
      "       'policy_deductable', 'policy_annual_premium', 'umbrella_limit',\n",
      "       'insured_zip', 'capital-gains', 'capital-loss',\n",
      "       ...\n",
      "       '1138', '1139', '1140', '1141', '1142', '1143', '1144', '1145', '1146',\n",
      "       'fraud_reported'],\n",
      "      dtype='object', length=1168)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "y = data_frame[\"fraud_reported\"]\n",
    "data_frame = data_frame.drop(columns = [\"fraud_reported\"])\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "string_features = data_frame.select_dtypes(include='object').columns\n",
    "encoded_data = encoder.fit_transform(data_frame[string_features])\n",
    "data_frame = data_frame.drop(columns=string_features)\n",
    "\n",
    "# concatenate encoded data with original dataframe\n",
    "data_frame = pd.concat([data_frame, pd.DataFrame(encoded_data.toarray())], axis=1)\n",
    "data_frame.columns= data_frame.columns.astype(str)\n",
    "print(data_frame.head())\n",
    "data_frame = pd.concat([data_frame, pd.DataFrame(y)])\n",
    "data_frame = data_frame.dropna()\n",
    "print(data_frame.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I decided to choose Logistic Regression as my baseline model.\n",
    "Lets fit this.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       months_as_customer  age  policy_number  policy_deductable  \\\n",
      "count                 0.0  0.0            0.0                0.0   \n",
      "mean                  NaN  NaN            NaN                NaN   \n",
      "std                   NaN  NaN            NaN                NaN   \n",
      "min                   NaN  NaN            NaN                NaN   \n",
      "25%                   NaN  NaN            NaN                NaN   \n",
      "50%                   NaN  NaN            NaN                NaN   \n",
      "75%                   NaN  NaN            NaN                NaN   \n",
      "max                   NaN  NaN            NaN                NaN   \n",
      "\n",
      "       policy_annual_premium  umbrella_limit  insured_zip  capital-gains  \\\n",
      "count                    0.0             0.0          0.0            0.0   \n",
      "mean                     NaN             NaN          NaN            NaN   \n",
      "std                      NaN             NaN          NaN            NaN   \n",
      "min                      NaN             NaN          NaN            NaN   \n",
      "25%                      NaN             NaN          NaN            NaN   \n",
      "50%                      NaN             NaN          NaN            NaN   \n",
      "75%                      NaN             NaN          NaN            NaN   \n",
      "max                      NaN             NaN          NaN            NaN   \n",
      "\n",
      "       capital-loss  incident_hour_of_the_day  ...  1135  1136  1137  1138  \\\n",
      "count           0.0                       0.0  ...   0.0   0.0   0.0   0.0   \n",
      "mean            NaN                       NaN  ...   NaN   NaN   NaN   NaN   \n",
      "std             NaN                       NaN  ...   NaN   NaN   NaN   NaN   \n",
      "min             NaN                       NaN  ...   NaN   NaN   NaN   NaN   \n",
      "25%             NaN                       NaN  ...   NaN   NaN   NaN   NaN   \n",
      "50%             NaN                       NaN  ...   NaN   NaN   NaN   NaN   \n",
      "75%             NaN                       NaN  ...   NaN   NaN   NaN   NaN   \n",
      "max             NaN                       NaN  ...   NaN   NaN   NaN   NaN   \n",
      "\n",
      "       1139  1140  1141  1142  1143  1144  \n",
      "count   0.0   0.0   0.0   0.0   0.0   0.0  \n",
      "mean    NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "std     NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "min     NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "25%     NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "50%     NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "75%     NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "max     NaN   NaN   NaN   NaN   NaN   NaN  \n",
      "\n",
      "[8 rows x 1163 columns]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mprint\u001b[39m(X\u001b[39m.\u001b[39mdescribe())\n\u001b[0;32m      3\u001b[0m y \u001b[39m=\u001b[39m data_frame[\u001b[39m\"\u001b[39m\u001b[39mfraud_reported\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m----> 4\u001b[0m X_train, X_test, y_train , y_test \u001b[39m=\u001b[39m train_test_split(X, y, test_size\u001b[39m=\u001b[39;49m\u001b[39m0.3\u001b[39;49m, random_state\u001b[39m=\u001b[39;49m\u001b[39m42\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Sanju\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2562\u001b[0m, in \u001b[0;36mtrain_test_split\u001b[1;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[0;32m   2559\u001b[0m arrays \u001b[39m=\u001b[39m indexable(\u001b[39m*\u001b[39marrays)\n\u001b[0;32m   2561\u001b[0m n_samples \u001b[39m=\u001b[39m _num_samples(arrays[\u001b[39m0\u001b[39m])\n\u001b[1;32m-> 2562\u001b[0m n_train, n_test \u001b[39m=\u001b[39m _validate_shuffle_split(\n\u001b[0;32m   2563\u001b[0m     n_samples, test_size, train_size, default_test_size\u001b[39m=\u001b[39;49m\u001b[39m0.25\u001b[39;49m\n\u001b[0;32m   2564\u001b[0m )\n\u001b[0;32m   2566\u001b[0m \u001b[39mif\u001b[39;00m shuffle \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[0;32m   2567\u001b[0m     \u001b[39mif\u001b[39;00m stratify \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Sanju\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_split.py:2236\u001b[0m, in \u001b[0;36m_validate_shuffle_split\u001b[1;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[0;32m   2233\u001b[0m n_train, n_test \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(n_train), \u001b[39mint\u001b[39m(n_test)\n\u001b[0;32m   2235\u001b[0m \u001b[39mif\u001b[39;00m n_train \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m-> 2236\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   2237\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWith n_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, test_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and train_size=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2238\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresulting train set will be empty. Adjust any of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2239\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39maforementioned parameters.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(n_samples, test_size, train_size)\n\u001b[0;32m   2240\u001b[0m     )\n\u001b[0;32m   2242\u001b[0m \u001b[39mreturn\u001b[39;00m n_train, n_test\n",
      "\u001b[1;31mValueError\u001b[0m: With n_samples=0, test_size=0.3 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
     ]
    }
   ],
   "source": [
    "\n",
    "X = data_frame.drop(columns=[\"fraud_reported\"])\n",
    "print(X.describe())\n",
    "y = data_frame[\"fraud_reported\"]\n",
    "X_train, X_test, y_train , y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Feature names are only supported if all input features have string names, but your input has ['int', 'str'] as feature name / column name types. If you want feature names to be stored and validated, you must convert them all to strings, by using X.columns = X.columns.astype(str) for example. Otherwise you can remove feature / column names from your input data, or convert them all to a non-string data type.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n",
      "File \u001b[1;32mc:\\Users\\Sanju\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1196\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1193\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1194\u001b[0m     _dtype \u001b[39m=\u001b[39m [np\u001b[39m.\u001b[39mfloat64, np\u001b[39m.\u001b[39mfloat32]\n\u001b[1;32m-> 1196\u001b[0m X, y \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(\n\u001b[0;32m   1197\u001b[0m     X,\n\u001b[0;32m   1198\u001b[0m     y,\n\u001b[0;32m   1199\u001b[0m     accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1200\u001b[0m     dtype\u001b[39m=\u001b[39;49m_dtype,\n\u001b[0;32m   1201\u001b[0m     order\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mC\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m   1202\u001b[0m     accept_large_sparse\u001b[39m=\u001b[39;49msolver \u001b[39mnot\u001b[39;49;00m \u001b[39min\u001b[39;49;00m [\u001b[39m\"\u001b[39;49m\u001b[39mliblinear\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msag\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39msaga\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[0;32m   1203\u001b[0m )\n\u001b[0;32m   1204\u001b[0m check_classification_targets(y)\n\u001b[0;32m   1205\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_ \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(y)\n",
      "File \u001b[1;32mc:\\Users\\Sanju\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:529\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    464\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_validate_data\u001b[39m(\n\u001b[0;32m    465\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[0;32m    466\u001b[0m     X\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mno_validation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcheck_params,\n\u001b[0;32m    471\u001b[0m ):\n\u001b[0;32m    472\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[0;32m    473\u001b[0m \n\u001b[0;32m    474\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    527\u001b[0m \u001b[39m        validated.\u001b[39;00m\n\u001b[0;32m    528\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 529\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_feature_names(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[0;32m    531\u001b[0m     \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_tags()[\u001b[39m\"\u001b[39m\u001b[39mrequires_y\u001b[39m\u001b[39m\"\u001b[39m]:\n\u001b[0;32m    532\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    533\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mThis \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m estimator \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    534\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrequires y to be passed, but the target y is None.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    535\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Sanju\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:396\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    376\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Set or check the `feature_names_in_` attribute.\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \n\u001b[0;32m    378\u001b[0m \u001b[39m.. versionadded:: 1.0\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[39m       should set `reset=False`.\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    395\u001b[0m \u001b[39mif\u001b[39;00m reset:\n\u001b[1;32m--> 396\u001b[0m     feature_names_in \u001b[39m=\u001b[39m _get_feature_names(X)\n\u001b[0;32m    397\u001b[0m     \u001b[39mif\u001b[39;00m feature_names_in \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    398\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_names_in_ \u001b[39m=\u001b[39m feature_names_in\n",
      "File \u001b[1;32mc:\\Users\\Sanju\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\validation.py:1903\u001b[0m, in \u001b[0;36m_get_feature_names\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m   1901\u001b[0m \u001b[39m# mixed type of string and non-string is not supported\u001b[39;00m\n\u001b[0;32m   1902\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(types) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mstr\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m types:\n\u001b[1;32m-> 1903\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   1904\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFeature names are only supported if all input features have string names, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1905\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut your input has \u001b[39m\u001b[39m{\u001b[39;00mtypes\u001b[39m}\u001b[39;00m\u001b[39m as feature name / column name types. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1906\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIf you want feature names to be stored and validated, you must convert \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1907\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthem all to strings, by using X.columns = X.columns.astype(str) for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1908\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mexample. Otherwise you can remove feature / column names from your input \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1909\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mdata, or convert them all to a non-string data type.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1910\u001b[0m     )\n\u001b[0;32m   1912\u001b[0m \u001b[39m# Only feature names of all strings are supported\u001b[39;00m\n\u001b[0;32m   1913\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(types) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m \u001b[39mand\u001b[39;00m types[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mstr\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "\u001b[1;31mTypeError\u001b[0m: Feature names are only supported if all input features have string names, but your input has ['int', 'str'] as feature name / column name types. If you want feature names to be stored and validated, you must convert them all to strings, by using X.columns = X.columns.astype(str) for example. Otherwise you can remove feature / column names from your input data, or convert them all to a non-string data type."
     ]
    }
   ],
   "source": [
    "model = model.fit(X_train, y_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
